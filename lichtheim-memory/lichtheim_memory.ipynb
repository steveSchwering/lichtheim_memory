{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steveSchwering/lichtheim_memory/blob/main/lichtheim-memory/lichtheim_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bklb2BxCfw-R"
      },
      "source": [
        "## General compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OncVkPtqughy",
        "outputId": "bf571af3-5c9c-4771-fb2d-7512e387a96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8wYJKdNuhEr",
        "outputId": "26c1b281-84b5-433c-e9a0-5c8a8dba1414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/dissertation/lichtheim-memory\n",
            "\u001b[0m\u001b[01;34m_deprecated\u001b[0m/            LichtheimMemory.py  lm_logging.py\n",
            "helperfunctions.py      lm_behavior.py      \u001b[01;34mmodel_info\u001b[0m/\n",
            "\u001b[01;34mlanguage\u001b[0m/               lm_dataloading.py   \u001b[01;34m__pycache__\u001b[0m/\n",
            "lichtheim-memory.ipynb  LMInterface.py      random_list_generator.py\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/dissertation/lichtheim-memory'\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HebWuysiuj7w",
        "outputId": "05b5d266-f317-487f-98cd-31302e1af4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cpu\n",
            "Do we have a GPU available?: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Current device: {torch.device(device)}')\n",
        "print(f'Do we have a GPU available?: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "  print(f'What is the current device on which we are loading the model?: {torch.cuda.current_device()}')\n",
        "  print(f'What is name of the current device?: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDAzwwAwoWDR"
      },
      "outputs": [],
      "source": [
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFWKY5HoeHO"
      },
      "source": [
        "## Reading in the artificial language and representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAnFA0egum93"
      },
      "source": [
        "The LMLInterface object is responsible for translating between sentences and the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqjrBf7Fu_uK"
      },
      "outputs": [],
      "source": [
        "import LMInterface\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Get language information\n",
        "language_name = 'datives'\n",
        "language_dir = Path.cwd().joinpath(f'language/{language_name}')\n",
        "\n",
        "interface = LMInterface.LMInterface(language_dir = language_dir,\n",
        "                                    phonology_filestem = 'phonology_artificial')\n",
        "\n",
        "# Generate sentences\n",
        "sentences_dir = Path.cwd().joinpath(f'language/{language_name}/all_sentences.tsv')\n",
        "sentences = LMInterface.utterances_from_file(sentences_dir, sep = \"\\t\")\n",
        "\n",
        "# Extract words\n",
        "words = list(interface.word_to_semantics.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwdFqSSLstB7",
        "outputId": "32ea2eea-ce73-4cee-eadf-ed0fe4aed6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "print(type(sentences[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J27zDfW1c2wc"
      },
      "source": [
        "## The Lichtheim-Memory model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-cL8gPkuIMS"
      },
      "outputs": [],
      "source": [
        "import LichtheimMemory    # Lichtheim-memory model\n",
        "\n",
        "import lm_dataloading     # Defines splitting train from test\n",
        "import lm_behavior        # Training functions and wrappers\n",
        "import lm_logging         # Functions to work with model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "futrQqKYR0mJ",
        "outputId": "d400836d-bd44-4774-8642-91dde5d56481"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'lm_logging' from '/content/drive/MyDrive/Colab Notebooks/dissertation/lichtheim-memory/lm_logging.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from importlib import reload\n",
        "\n",
        "reload(LMInterface)\n",
        "reload(LichtheimMemory)\n",
        "reload(lm_dataloading)\n",
        "reload(lm_behavior)\n",
        "reload(lm_logging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhKHA-KD62Eq"
      },
      "source": [
        "## Instantiation and training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9E0pM494p6a"
      },
      "outputs": [],
      "source": [
        "def instantiate_model_and_environment(words, utterances, tt_split, seed, interface,\n",
        "                                      phh1_size, wsh1_size, wsh2_size, ssh1_size, ssh2_size, ssh3_size,\n",
        "                                      lr, weight_decay, lr_steprate, lr_gamma,\n",
        "                                      sample_tt_probability = True, bin_tt_label = 'probability',\n",
        "                                      save_init_model = True):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # Generate training and testing sets for the model. Save them to disk.\n",
        "  dl_words, dl_sent_train, dl_sent_test, sent_train, sent_test = lm_dataloading.inst_training_env(words = words,\n",
        "                                                                                                  utterances = sentences,\n",
        "                                                                                                  tt_split = tt_split,\n",
        "                                                                                                  interface = interface,\n",
        "                                                                                                  sample_tt_probability = sample_tt_probability,\n",
        "                                                                                                  bin_tt_label = bin_tt_label,\n",
        "                                                                                                  seed = seed)\n",
        "\n",
        "  # Generate model, along with optimizer and scheduler\n",
        "  LM, optimizer, scheduler = LichtheimMemory.inst_LM(interface = interface,\n",
        "                                                      phh1_size = phh1_size, wsh1_size = wsh1_size, wsh2_size = wsh2_size,\n",
        "                                                      ssh1_size = ssh1_size, ssh2_size = ssh2_size, ssh3_size = ssh3_size,\n",
        "                                                      lr = lr, weight_decay = weight_decay, lr_steprate = lr_steprate, lr_gamma = lr_gamma,\n",
        "                                                      seed = seed)\n",
        "\n",
        "  # Log the initialization of the model\n",
        "  if save_init_model:\n",
        "    lm_logging.log_model_init(model = LM,\n",
        "                              language_dir = interface.language_dir,\n",
        "                              seed = seed,\n",
        "                              sent_train = sent_train,\n",
        "                              sent_test = sent_test)\n",
        "\n",
        "  # Save the initialized model\n",
        "  training_meta_info = {'language' : interface.language_dir, 'seed' : seed, 'sample_tt_probability' : sample_tt_probability, 'bin_label' : bin_tt_label,\n",
        "                        'lr' : lr, 'weight_decay' : weight_decay, 'lr_steprate' : lr_steprate, 'lr_gamma' : lr_gamma,\n",
        "                        'word_epochs_per_sentence_epoch' : word_epochs_per_sentence_epoch, 'word_epochs_decay_stepsize' : word_epochs_decay_stepsize, 'word_epochs_decay_steprate' : word_epochs_decay_steprate}\n",
        "  if save_init_model:\n",
        "    lm_logging.log_model_state(model = LM,\n",
        "                               trained_epochs = \"init\",\n",
        "                               training_meta_info = training_meta_info)\n",
        "\n",
        "  return dl_words, dl_sent_train, dl_sent_test, sent_train, sent_test, LM, optimizer, scheduler, training_meta_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYmpWTqis59M"
      },
      "outputs": [],
      "source": [
        "def inst_and_train_model(words, utterances, tt_split, interface, seed,\n",
        "                         phh1_size, wsh1_size, wsh2_size, ssh1_size, ssh2_size, ssh3_size,\n",
        "                         lr, weight_decay, lr_steprate, lr_gamma,\n",
        "                         num_train_epochs,\n",
        "                         word_epochs_per_sentence_epoch, word_epochs_decay_stepsize, word_epochs_decay_steprate,\n",
        "                         report_every, summarize_every, log_every,\n",
        "                         sample_tt_probability = True, bin_tt_label = 'structure',\n",
        "                         save_init_model = True):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  dl_words, dl_sent_train, dl_sent_test, sent_train, sent_test, LM, optimizer, scheduler, training_meta_info = instantiate_model_and_environment(words = words, utterances = utterances, tt_split = tt_split, interface = interface, seed = seed,\n",
        "                                                                                                                                                 phh1_size = phh1_size, wsh1_size = wsh1_size, wsh2_size = wsh2_size, ssh1_size = ssh1_size, ssh2_size = ssh2_size, ssh3_size = ssh3_size,\n",
        "                                                                                                                                                 lr = lr, weight_decay = weight_decay, lr_steprate = lr_steprate, lr_gamma = lr_gamma,\n",
        "                                                                                                                                                 sample_tt_probability = sample_tt_probability, bin_tt_label = bin_tt_label,\n",
        "                                                                                                                                                 save_init_model = save_init_model)\n",
        "\n",
        "  # Train the model\n",
        "  all_loss_word, all_loss_sentence, training_meta_info = lm_behavior.interleave_training(model = LM,\n",
        "                                                                                          dataloader_sentences = dl_sent_train,\n",
        "                                                                                          dataloader_words = dl_words,\n",
        "                                                                                          optimizer = optimizer,\n",
        "                                                                                          scheduler = scheduler,\n",
        "                                                                                          num_epochs = num_train_epochs,\n",
        "                                                                                          report_every = report_every,\n",
        "                                                                                          summarize_every = summarize_every,\n",
        "                                                                                          log_every = log_every,\n",
        "                                                                                          training_meta_info = training_meta_info,\n",
        "                                                                                          word_epochs_per_sentence_epoch = word_epochs_per_sentence_epoch,\n",
        "                                                                                          word_epochs_decay_stepsize = word_epochs_decay_stepsize,\n",
        "                                                                                          word_epochs_decay_steprate = word_epochs_decay_steprate)\n",
        "\n",
        "  # Final log of model to ensure it is saved\n",
        "  # NOTE USE OF num_train_epochs - 1 TO CALCULATE TRAINED_EPOCHS NUM\n",
        "  lm_logging.log_model_state(model = LM,\n",
        "                             trained_epochs = num_train_epochs - 1,\n",
        "                             training_meta_info = training_meta_info)\n",
        "\n",
        "  return all_loss_word, all_loss_sentence, training_meta_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loukIFucKcJB"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "srbDf7Poy5a1",
        "outputId": "1e13ce4b-c42c-400a-9191-a46dc68a04e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5\n",
            "\tRepetition -- Loss ph: 0.15398992764453093\n",
            "\tRepetition -- Loss ws: 0.09609452458719413\n",
            "\tRepetition -- Loss ss: 0.3139335254828135\n",
            "\tComprehension -- Loss ph: 0.10174027227693135\n",
            "\tComprehension -- Loss ws: 0.08856318487889236\n",
            "\tComprehension -- Loss ss: 0.09075367015217327\n",
            "\tProduction -- Loss ph: 0.10324317836978783\n",
            "\tProduction -- Loss ws: 0.08277741681784391\n",
            "\tProduction -- Loss ss: 0.3007949549208085\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 10\n",
            "\tRepetition -- Loss ph: 0.1374197501440843\n",
            "\tRepetition -- Loss ws: 0.11333197837074598\n",
            "\tRepetition -- Loss ss: 0.3300779587527116\n",
            "\tComprehension -- Loss ph: 0.10157636635005474\n",
            "\tComprehension -- Loss ws: 0.09869203493412998\n",
            "\tComprehension -- Loss ss: 0.08279393411137992\n",
            "\tProduction -- Loss ph: 0.0910449882565687\n",
            "\tProduction -- Loss ws: 0.08354807146514455\n",
            "\tProduction -- Loss ss: 0.31731464078029\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 15\n",
            "\tRepetition -- Loss ph: 0.13220516314109165\n",
            "\tRepetition -- Loss ws: 0.1236138163258632\n",
            "\tRepetition -- Loss ss: 0.3531607384483019\n",
            "\tComprehension -- Loss ph: 0.10280002259545856\n",
            "\tComprehension -- Loss ws: 0.10518290672037336\n",
            "\tComprehension -- Loss ss: 0.08021411366036368\n",
            "\tProduction -- Loss ph: 0.07581148641804854\n",
            "\tProduction -- Loss ws: 0.08575024992848436\n",
            "\tProduction -- Loss ss: 0.3233920878171921\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 20\n",
            "\tRepetition -- Loss ph: 0.1147117226322492\n",
            "\tRepetition -- Loss ws: 0.1268358282248179\n",
            "\tRepetition -- Loss ss: 0.33748772407571476\n",
            "\tComprehension -- Loss ph: 0.09821135902570353\n",
            "\tComprehension -- Loss ws: 0.10863124380095138\n",
            "\tComprehension -- Loss ss: 0.07777283008520802\n",
            "\tProduction -- Loss ph: 0.06646542504119377\n",
            "\tProduction -- Loss ws: 0.093234068552653\n",
            "\tProduction -- Loss ss: 0.3288151529679696\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 25\n",
            "\tRepetition -- Loss ph: 0.105698413575689\n",
            "\tRepetition -- Loss ws: 0.1456066002200047\n",
            "\tRepetition -- Loss ss: 0.3404841810464859\n",
            "\tComprehension -- Loss ph: 0.09699020613398816\n",
            "\tComprehension -- Loss ws: 0.11512940555810929\n",
            "\tComprehension -- Loss ss: 0.07702697758976784\n",
            "\tProduction -- Loss ph: 0.0514903494506143\n",
            "\tProduction -- Loss ws: 0.09622981724639734\n",
            "\tProduction -- Loss ss: 0.3430346546322107\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 30\n",
            "\tRepetition -- Loss ph: 0.09972926994164785\n",
            "\tRepetition -- Loss ws: 0.14466891462604206\n",
            "\tRepetition -- Loss ss: 0.32990224331617357\n",
            "\tComprehension -- Loss ph: 0.09715677665339575\n",
            "\tComprehension -- Loss ws: 0.11187873654067516\n",
            "\tComprehension -- Loss ss: 0.07331196250704428\n",
            "\tProduction -- Loss ph: 0.04652029238486042\n",
            "\tProduction -- Loss ws: 0.09236657653003931\n",
            "\tProduction -- Loss ss: 0.3440677833308776\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 35\n",
            "\tRepetition -- Loss ph: 0.08301160740355651\n",
            "\tRepetition -- Loss ws: 0.15891595947245757\n",
            "\tRepetition -- Loss ss: 0.3482879543801149\n",
            "\tComprehension -- Loss ph: 0.09496041263143222\n",
            "\tComprehension -- Loss ws: 0.12770135800043741\n",
            "\tComprehension -- Loss ss: 0.07611554575877057\n",
            "\tProduction -- Loss ph: 0.040916862339169406\n",
            "\tProduction -- Loss ws: 0.11022701161603132\n",
            "\tProduction -- Loss ss: 0.36556819967925547\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 40\n",
            "\tRepetition -- Loss ph: 0.07580274612953265\n",
            "\tRepetition -- Loss ws: 0.16959976146618524\n",
            "\tRepetition -- Loss ss: 0.3472986900806427\n",
            "\tComprehension -- Loss ph: 0.094043723543485\n",
            "\tComprehension -- Loss ws: 0.1379643581310908\n",
            "\tComprehension -- Loss ss: 0.07118723614865707\n",
            "\tProduction -- Loss ph: 0.03168013408624878\n",
            "\tProduction -- Loss ws: 0.1287917565802733\n",
            "\tProduction -- Loss ss: 0.36400339792172115\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 45\n",
            "\tRepetition -- Loss ph: 0.07095373907436928\n",
            "\tRepetition -- Loss ws: 0.18180717542767524\n",
            "\tRepetition -- Loss ss: 0.3578239239752293\n",
            "\tComprehension -- Loss ph: 0.09426088470551702\n",
            "\tComprehension -- Loss ws: 0.14799231647617286\n",
            "\tComprehension -- Loss ss: 0.06523325209195416\n",
            "\tProduction -- Loss ph: 0.032171510576930205\n",
            "\tProduction -- Loss ws: 0.14436408085127672\n",
            "\tProduction -- Loss ss: 0.3630063171684742\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 50\n",
            "\tRepetition -- Loss ph: 0.06116560634225607\n",
            "\tRepetition -- Loss ws: 0.20694986303647359\n",
            "\tRepetition -- Loss ss: 0.35688615689675013\n",
            "\tComprehension -- Loss ph: 0.09289639293319649\n",
            "\tComprehension -- Loss ws: 0.16486139478782813\n",
            "\tComprehension -- Loss ss: 0.06499144161120057\n",
            "\tProduction -- Loss ph: 0.032075036763756844\n",
            "\tProduction -- Loss ws: 0.16071667738258838\n",
            "\tProduction -- Loss ss: 0.3746002048254013\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 55\n",
            "\tRepetition -- Loss ph: 0.057636566112438836\n",
            "\tRepetition -- Loss ws: 0.20639827753106754\n",
            "\tRepetition -- Loss ss: 0.35446999698877335\n",
            "\tComprehension -- Loss ph: 0.0920133726629946\n",
            "\tComprehension -- Loss ws: 0.15856875461836656\n",
            "\tComprehension -- Loss ss: 0.06249683472431368\n",
            "\tProduction -- Loss ph: 0.02333704180608038\n",
            "\tProduction -- Loss ws: 0.1566632423053185\n",
            "\tProduction -- Loss ss: 0.37700536996126177\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 60\n",
            "\tRepetition -- Loss ph: 0.05819061279296875\n",
            "\tRepetition -- Loss ws: 0.20918681214253107\n",
            "\tRepetition -- Loss ss: 0.3659807176391284\n",
            "\tComprehension -- Loss ph: 0.09211491460601488\n",
            "\tComprehension -- Loss ws: 0.1626590407308605\n",
            "\tComprehension -- Loss ss: 0.061489037209086946\n",
            "\tProduction -- Loss ph: 0.02394860938792893\n",
            "\tProduction -- Loss ws: 0.16044781362016997\n",
            "\tProduction -- Loss ss: 0.3743699056158463\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 65\n",
            "\tRepetition -- Loss ph: 0.0591400737563769\n",
            "\tRepetition -- Loss ws: 0.2069127940138181\n",
            "\tRepetition -- Loss ss: 0.36785751074552536\n",
            "\tComprehension -- Loss ph: 0.09230075702899032\n",
            "\tComprehension -- Loss ws: 0.1620563377191623\n",
            "\tComprehension -- Loss ss: 0.0620616968224446\n",
            "\tProduction -- Loss ph: 0.026434602038934826\n",
            "\tProduction -- Loss ws: 0.1595200525969267\n",
            "\tProduction -- Loss ss: 0.37710252473751704\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 70\n",
            "\tRepetition -- Loss ph: 0.0573389562095205\n",
            "\tRepetition -- Loss ws: 0.21078761219978331\n",
            "\tRepetition -- Loss ss: 0.36926782459020613\n",
            "\tComprehension -- Loss ph: 0.09220263018376297\n",
            "\tComprehension -- Loss ws: 0.16362856574356555\n",
            "\tComprehension -- Loss ss: 0.06003137081447575\n",
            "\tProduction -- Loss ph: 0.022097556425530154\n",
            "\tProduction -- Loss ws: 0.16017705025772253\n",
            "\tProduction -- Loss ss: 0.3760579893986384\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 75\n",
            "\tRepetition -- Loss ph: 0.05735094703733921\n",
            "\tRepetition -- Loss ws: 0.2069980899989605\n",
            "\tRepetition -- Loss ss: 0.3671274165312449\n",
            "\tComprehension -- Loss ph: 0.09202171995408005\n",
            "\tComprehension -- Loss ws: 0.1603139538400703\n",
            "\tComprehension -- Loss ss: 0.05760292115310828\n",
            "\tProduction -- Loss ph: 0.021034816499450243\n",
            "\tProduction -- Loss ws: 0.15858307274679342\n",
            "\tProduction -- Loss ss: 0.3809979462871949\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 80\n",
            "\tRepetition -- Loss ph: 0.05582600716501474\n",
            "\tRepetition -- Loss ws: 0.20662946472565333\n",
            "\tRepetition -- Loss ss: 0.3682063193122546\n",
            "\tComprehension -- Loss ph: 0.09201899948219458\n",
            "\tComprehension -- Loss ws: 0.16054099816415046\n",
            "\tComprehension -- Loss ss: 0.05803498875556721\n",
            "\tProduction -- Loss ph: 0.01859272570601509\n",
            "\tProduction -- Loss ws: 0.15975479168196519\n",
            "\tProduction -- Loss ss: 0.3810603857288758\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 85\n",
            "\tRepetition -- Loss ph: 0.05596336764593919\n",
            "\tRepetition -- Loss ws: 0.20693302462498347\n",
            "\tRepetition -- Loss ss: 0.3677258762717247\n",
            "\tComprehension -- Loss ph: 0.09208429456584984\n",
            "\tComprehension -- Loss ws: 0.16187398304541906\n",
            "\tComprehension -- Loss ss: 0.057048691991302705\n",
            "\tProduction -- Loss ph: 0.017425087104396275\n",
            "\tProduction -- Loss ws: 0.160301708355546\n",
            "\tProduction -- Loss ss: 0.3812345103174448\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 90\n",
            "\tRepetition -- Loss ph: 0.05622840449213982\n",
            "\tRepetition -- Loss ws: 0.2088285688062509\n",
            "\tRepetition -- Loss ss: 0.36122411727905274\n",
            "\tComprehension -- Loss ph: 0.09210400056507852\n",
            "\tComprehension -- Loss ws: 0.16540198971827824\n",
            "\tComprehension -- Loss ss: 0.05623485684187876\n",
            "\tProduction -- Loss ph: 0.01836288128096688\n",
            "\tProduction -- Loss ws: 0.16196835279464722\n",
            "\tProduction -- Loss ss: 0.3721803528567155\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 95\n",
            "\tRepetition -- Loss ph: 0.05572144988924265\n",
            "\tRepetition -- Loss ws: 0.20576499248544375\n",
            "\tRepetition -- Loss ss: 0.351632154583931\n",
            "\tComprehension -- Loss ph: 0.09190933309495448\n",
            "\tComprehension -- Loss ws: 0.16433952430884044\n",
            "\tComprehension -- Loss ss: 0.05566898427903652\n",
            "\tProduction -- Loss ph: 0.017897959075053222\n",
            "\tProduction -- Loss ws: 0.16276533633470536\n",
            "\tProduction -- Loss ss: 0.37659200412531696\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 100\n",
            "\tRepetition -- Loss ph: 0.05532406528790792\n",
            "\tRepetition -- Loss ws: 0.20307879308859508\n",
            "\tRepetition -- Loss ss: 0.3598969406882922\n",
            "\tComprehension -- Loss ph: 0.09194039088984331\n",
            "\tComprehension -- Loss ws: 0.16527775330675973\n",
            "\tComprehension -- Loss ss: 0.05785566965738932\n",
            "\tProduction -- Loss ph: 0.022193726696035202\n",
            "\tProduction -- Loss ws: 0.16340941901008288\n",
            "\tProduction -- Loss ss: 0.37761014051735403\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 105\n",
            "\tRepetition -- Loss ph: 0.055072102112074695\n",
            "\tRepetition -- Loss ws: 0.20580291286110877\n",
            "\tRepetition -- Loss ss: 0.368913929661115\n",
            "\tComprehension -- Loss ph: 0.09180117108755642\n",
            "\tComprehension -- Loss ws: 0.1666621171269152\n",
            "\tComprehension -- Loss ss: 0.05588639438773195\n",
            "\tProduction -- Loss ph: 0.019813186660564194\n",
            "\tProduction -- Loss ws: 0.16609364474813143\n",
            "\tProduction -- Loss ss: 0.37514731466770174\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 110\n",
            "\tRepetition -- Loss ph: 0.05497086172302564\n",
            "\tRepetition -- Loss ws: 0.2059289488196373\n",
            "\tRepetition -- Loss ss: 0.3656923617919286\n",
            "\tComprehension -- Loss ph: 0.0916397941029734\n",
            "\tComprehension -- Loss ws: 0.16613302199376953\n",
            "\tComprehension -- Loss ss: 0.05746659555989835\n",
            "\tProduction -- Loss ph: 0.020356019261525943\n",
            "\tProduction -- Loss ws: 0.1637181588013967\n",
            "\tProduction -- Loss ss: 0.37246998615562915\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 115\n",
            "\tRepetition -- Loss ph: 0.05439056766529878\n",
            "\tRepetition -- Loss ws: 0.2082505202293396\n",
            "\tRepetition -- Loss ss: 0.3711351471145948\n",
            "\tComprehension -- Loss ph: 0.09188449140224192\n",
            "\tComprehension -- Loss ws: 0.1668205340206623\n",
            "\tComprehension -- Loss ss: 0.05699164483282301\n",
            "\tProduction -- Loss ph: 0.01649679728570239\n",
            "\tProduction -- Loss ws: 0.16521193951368332\n",
            "\tProduction -- Loss ss: 0.3744881365696589\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 120\n",
            "\tRepetition -- Loss ph: 0.05498224663237731\n",
            "\tRepetition -- Loss ws: 0.2045279482503732\n",
            "\tRepetition -- Loss ss: 0.35643756061792375\n",
            "\tComprehension -- Loss ph: 0.09157065961096023\n",
            "\tComprehension -- Loss ws: 0.1670435012049145\n",
            "\tComprehension -- Loss ss: 0.055973254131774106\n",
            "\tProduction -- Loss ph: 0.020048776184169885\n",
            "\tProduction -- Loss ws: 0.16437364598115287\n",
            "\tProduction -- Loss ss: 0.37998543282349906\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 125\n",
            "\tRepetition -- Loss ph: 0.054573202120761076\n",
            "\tRepetition -- Loss ws: 0.20335186183452605\n",
            "\tRepetition -- Loss ss: 0.3556226842602094\n",
            "\tComprehension -- Loss ph: 0.09172540376583735\n",
            "\tComprehension -- Loss ws: 0.16382819700572226\n",
            "\tComprehension -- Loss ss: 0.05601058108939065\n",
            "\tProduction -- Loss ph: 0.019092127280697847\n",
            "\tProduction -- Loss ws: 0.1639004008968671\n",
            "\tProduction -- Loss ss: 0.3824190532167753\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n",
            "Epoch 130\n",
            "\tRepetition -- Loss ph: 0.05515350102136533\n",
            "\tRepetition -- Loss ws: 0.20409904912114143\n",
            "\tRepetition -- Loss ss: 0.3648112818598747\n",
            "\tComprehension -- Loss ph: 0.09177817872828907\n",
            "\tComprehension -- Loss ws: 0.1646812636156877\n",
            "\tComprehension -- Loss ss: 0.05578344395591153\n",
            "\tProduction -- Loss ph: 0.01964506421451612\n",
            "\tProduction -- Loss ws: 0.1647471743822098\n",
            "\tProduction -- Loss ss: 0.3765845697373152\n",
            "\tRepetition_Word -- NA\n",
            "\tComprehension_Word -- NA\n",
            "\tProduction_Word -- NA\n"
          ]
        }
      ],
      "source": [
        "tt_split = 0.75\n",
        "seed = 128\n",
        "sample_tt_probability = True\n",
        "bin_tt_label = 'structure'\n",
        "\n",
        "phh1_size = 200\n",
        "wsh1_size = 100\n",
        "wsh2_size = 100\n",
        "ssh1_size = 100\n",
        "ssh2_size = 200\n",
        "ssh3_size = 100\n",
        "\n",
        "lr = 0.50\n",
        "weight_decay = 10e-6\n",
        "lr_steprate = 50\n",
        "lr_gamma = 0.1\n",
        "\n",
        "word_epochs_per_sentence_epoch = 3\n",
        "word_epochs_decay_stepsize = 1\n",
        "word_epochs_decay_steprate = 10\n",
        "\n",
        "num_train_epochs = 200\n",
        "\n",
        "log_every = 1\n",
        "report_every = num_train_epochs + 1\n",
        "summarize_every = 5\n",
        "\n",
        "o = inst_and_train_model(words = words, utterances = sentences, tt_split = tt_split, interface = interface, seed = seed, sample_tt_probability = sample_tt_probability, bin_tt_label = bin_tt_label,\n",
        "                          phh1_size = phh1_size, wsh1_size = wsh1_size, wsh2_size = wsh2_size, ssh1_size = ssh1_size, ssh2_size = ssh2_size, ssh3_size = ssh3_size,\n",
        "                          lr = lr, weight_decay = weight_decay, lr_steprate = lr_steprate, lr_gamma = lr_gamma,\n",
        "                          word_epochs_per_sentence_epoch = word_epochs_per_sentence_epoch, word_epochs_decay_stepsize = word_epochs_decay_stepsize, word_epochs_decay_steprate = word_epochs_decay_steprate,\n",
        "                          num_train_epochs = num_train_epochs,\n",
        "                          log_every = log_every, report_every = report_every, summarize_every = summarize_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhvAeL01BUNh"
      },
      "source": [
        "## Examining model behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzxg-vYSD4M6"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import LMInterface    # For reading in the sentences from file\n",
        "import lm_dataloading # For converting those sentences into a Corpus\n",
        "\n",
        "from torch.utils.data import DataLoader # For converting the Corpus into a Dataloader\n",
        "from torch import load                  # Loading pre-trained model\n",
        "from pathlib import Path                # Path stuff\n",
        "\n",
        "\n",
        "def load_nonword_memory_lists(language, interface,\n",
        "                               nonword_legality = 'legal',\n",
        "                               num_lists = 500,\n",
        "                               list_length = 4,\n",
        "                               task_weights = {'repetition' : 1},\n",
        "                               sep = '\\t'):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # Read in nonword memory lists as a list of dictionaries\n",
        "  memory_list_dir = Path.cwd().joinpath(f'language/{language}/nonword_memory_lists_{nonword_legality}_length{list_length}_n{num_lists}.tsv')\n",
        "  memory_lists = LMInterface.utterances_from_file(utterances_file = memory_list_dir, sep = sep)\n",
        "\n",
        "  # Generate Corpus object of the memory lists\n",
        "  corpus_memory_lists = lm_dataloading.Corpus(utterances = [l['utterance_lst'] for l in memory_lists],\n",
        "                                              utterance_infos = [l.to_dict() for l in memory_lists],\n",
        "                                              interface = interface,\n",
        "                                              task_weights = task_weights)\n",
        "\n",
        "  # Generate dataloader\n",
        "  dataloader_nwml = DataLoader(corpus_memory_lists,\n",
        "                               batch_size = None)\n",
        "\n",
        "  return dataloader_nwml\n",
        "\n",
        "def load_noun_memory_lists(language, interface,\n",
        "                           num_lists = 500,\n",
        "                           list_length = 4,\n",
        "                           task_weights = {'repetition' : 1},\n",
        "                           sep = '\\t'):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # Read in noun memory lists as a list of dictionaries\n",
        "  memory_list_dir = Path.cwd().joinpath(f'language/{language}/noun_memory_lists_length{list_length}_n{num_lists}.tsv')\n",
        "  memory_lists = LMInterface.utterances_from_file(utterances_file = memory_list_dir, sep = sep)\n",
        "\n",
        "  # Generate Corpus object of the memory lists\n",
        "  corpus_memory_lists = lm_dataloading.Corpus(utterances = [l['utterance_lst'] for l in memory_lists],\n",
        "                                              utterance_infos = [l.to_dict() for l in memory_lists],\n",
        "                                              interface = interface,\n",
        "                                              task_weights = task_weights)\n",
        "\n",
        "  # Generate dataloader\n",
        "  dataloader_nml = DataLoader(corpus_memory_lists,\n",
        "                              batch_size = None)\n",
        "\n",
        "  return dataloader_nml\n",
        "\n",
        "def load_scrambled_sentences(language, interface,\n",
        "                             task_weights = {'repetition' : 1},\n",
        "                             sep = '\\t'):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # Read in scrambled sentences as a list of dictionaries\n",
        "  scrambled_sentence_dir = Path.cwd().joinpath(f'language/{language}/scrambled_sentences.tsv')\n",
        "  scrambled_sentences = LMInterface.utterances_from_file(utterances_file = scrambled_sentence_dir, sep = sep)\n",
        "\n",
        "  # Generate Corpus object of scrambled sentences\n",
        "  corpus_scrambled_sentences = lm_dataloading.Corpus(utterances = [l['utterance_lst'] for l in scrambled_sentences],\n",
        "                                                     utterance_infos = [s.to_dict() for s in scrambled_sentences],\n",
        "                                                     interface = interface,\n",
        "                                                     task_weights = task_weights)\n",
        "\n",
        "  # Generate dataloader\n",
        "  dataloder_scrambled = DataLoader(corpus_scrambled_sentences,\n",
        "                                   batch_size = None)\n",
        "\n",
        "  return dataloder_scrambled\n",
        "\n",
        "def load_lexsyn_violations(language, interface,\n",
        "                           violation = 'vb_inn',\n",
        "                           task_weights = {'repetition' : 1},\n",
        "                           sep = '\\t'):\n",
        "  \"\"\"\n",
        "  In this case, we also want to check whether the specific sentence being changed is in the training set\n",
        "  \"\"\"\n",
        "  # Read in lexico-syntactic violations as a list of dictionaries\n",
        "  lexsyn_violations_dir = Path.cwd().joinpath(f'language/{language}/sentence_{violation}.tsv')\n",
        "  lexsyn_violations = LMInterface.utterances_from_file(utterances_file = lexsyn_violations_dir, sep = sep)\n",
        "\n",
        "  # Generate Corpus object of lexico-syntactic violations\n",
        "  corpus_lexsyn_violations = lm_dataloading.Corpus(utterances = [l['utterance_lst'] for l in lexsyn_violations],\n",
        "                                                   utterance_infos = [s.to_dict() for s in lexsyn_violations],\n",
        "                                                   interface = interface,\n",
        "                                                   task_weights = task_weights)\n",
        "\n",
        "  # Generate dataloader\n",
        "  dataloader_lexsyn_violations = DataLoader(corpus_lexsyn_violations,\n",
        "                                            batch_size = None)\n",
        "  return dataloader_lexsyn_violations\n",
        "\n",
        "def load_model_env(model_info_path,\n",
        "                   task_weights = {'repetition' : 1, 'comprehension' : 1, 'production' : 1},\n",
        "                   sep = ','):\n",
        "  \"\"\"\n",
        "  Read in training and testing dataloaders for a mdoel\n",
        "  \"\"\"\n",
        "  training_env_path = model_info_path.joinpath('train_corpus_info.csv')\n",
        "  testing_env_path = model_info_path.joinpath('test_corpus_info.csv')\n",
        "\n",
        "  training_env = LMInterface.utterances_from_file(utterances_file = training_env_path, sep = sep)\n",
        "  testing_env = LMInterface.utterances_from_file(utterances_file = testing_env_path, sep = sep)\n",
        "\n",
        "  corpus_sentences_training = lm_dataloading.Corpus(utterances = [sentence['sentence_lst'] for sentence in training_env],\n",
        "                                                    utterance_infos = training_env,\n",
        "                                                    interface = interface,\n",
        "                                                    task_weights = task_weights)\n",
        "  corpus_sentences_testing = lm_dataloading.Corpus(utterances = [sentence['sentence_lst'] for sentence in testing_env],\n",
        "                                                   utterance_infos = testing_env,\n",
        "                                                   interface = interface,\n",
        "                                                   task_weights = task_weights)\n",
        "\n",
        "  dataloader_sentences_training = DataLoader(corpus_sentences_training,\n",
        "                                             batch_size = None)\n",
        "  dataloader_sentences_testing = DataLoader(corpus_sentences_testing,\n",
        "                                            batch_size = None)\n",
        "\n",
        "  return dataloader_sentences_training, dataloader_sentences_testing, training_env, testing_env\n",
        "\n",
        "def load_model(interface, model_info_path, trained_epochs):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  model_id = model_info_path.stem\n",
        "\n",
        "  # Load in model\n",
        "  # -- Access model log to get info about trained model, identifying which model is trained trained_epochs times\n",
        "  # -- If multiple rows selected (i.e. model saved multiple times at same epoch) select last row\n",
        "  model_log_path = model_info_path.joinpath('model_log.csv')\n",
        "  model_log = pd.read_csv(model_log_path)\n",
        "  assert trained_epochs in model_log['trained_epochs'].values # Assert the model is trained\n",
        "  model_info = model_log.loc[model_log['trained_epochs'] == trained_epochs] # Here we select the model\n",
        "  if model_info.shape[0] > 1:\n",
        "    model_info = model_info.iloc[-1]\n",
        "\n",
        "  # -- Create model and read in the weights\n",
        "  model, optimizer, scheduler = LichtheimMemory.inst_LM(interface = interface,\n",
        "                                                        phh1_size = model_info['phh1_size'].item(),\n",
        "                                                        wsh1_size = model_info['wsh1_size'].item(), wsh2_size = model_info['wsh2_size'].item(),\n",
        "                                                        ssh1_size = model_info['ssh1_size'].item(), ssh2_size = model_info['ssh2_size'].item(), ssh3_size = model_info['ssh3_size'].item(),\n",
        "                                                        seed = model_info['seed'].item(),\n",
        "                                                        lr = model_info['lr'].item(), weight_decay = model_info['weight_decay'].item(), lr_steprate = model_info['lr_steprate'].item(), lr_gamma = model_info['lr_gamma'].item(),\n",
        "                                                        model_id = model_id)\n",
        "\n",
        "  # -- Identify the path to the trained model\n",
        "  try:\n",
        "    model_trained_path = Path(model_info['model_object_path'].item())\n",
        "  except AttributeError:\n",
        "    model_trained_path = Path(model_info['model_object_path'])\n",
        "  model.load_state_dict(load(model_trained_path))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeYRJ8eSwuMW"
      },
      "outputs": [],
      "source": [
        "import lm_behavior\n",
        "import lm_logging\n",
        "\n",
        "def test_model_performance(model, dataloader, interface, behavior_type,\n",
        "                           meta_info = {'trained_epochs' : None}):\n",
        "  \"\"\"\n",
        "  Test model performance on a generic task, with input passed in through datalaoder object\n",
        "  \"\"\"\n",
        "  # Get losses and behaviors\n",
        "  behaviors, loss = lm_behavior.test_epoch(model = model,\n",
        "                                           dataloader = dataloader)\n",
        "\n",
        "  # Package output of behavior\n",
        "  behaviors = lm_logging.package_all_output(behavior = behaviors,\n",
        "                                            interface = interface)\n",
        "\n",
        "  # Log all losses and behaviors\n",
        "  lm_logging.log_model_behavior(model = model,\n",
        "                                behavior = behaviors,\n",
        "                                trained_epochs = meta_info['trained_epochs'],\n",
        "                                behavior_type = behavior_type,\n",
        "                                meta_info = meta_info)\n",
        "  lm_logging.log_model_loss(model = model,\n",
        "                            loss = loss,\n",
        "                            trained_epochs = meta_info['trained_epochs'],\n",
        "                            behavior_type = behavior_type,\n",
        "                            meta_info = meta_info)\n",
        "\n",
        "def test_multiple_model_performances(model, task_to_dataloaders, interface,\n",
        "                                     meta_info = {'trained_epochs' : None}):\n",
        "  \"\"\"\n",
        "  Tasks to dataloaders a dictionary of tasks and the dataloaders associated\n",
        "  with those tasks\n",
        "  \"\"\"\n",
        "  for task in task_to_dataloaders:\n",
        "    test_model_performance(model = model,\n",
        "                           dataloader = task_to_dataloaders[task],\n",
        "                           interface = interface,\n",
        "                           behavior_type = task,\n",
        "                           meta_info = meta_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwOEAZbnxSQ0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def unique(sequence):\n",
        "    seen = set()\n",
        "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
        "\n",
        "def load_task_dataloaders(interface, model_path,\n",
        "                          memory_list_interface = None,\n",
        "                          tasks = ['train', 'test', 'scrambled_sentence', 'nonword_list', 'single_nonword_repetition',\n",
        "                                   'lexsyn_violation_vb_trn', 'lexsyn_violation_vb_int', 'lexsyn_violation_vb_inn', 'lexsyn_violation_snb_inn', 'lexsyn_violation_pvnb_inn'],\n",
        "                          nonword_legality = 'legal'):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # -- Model environment needs to be loaded once\n",
        "  dataloader_sentences_training, dataloader_sentences_testing, training_env, testing_env = load_model_env(model_info_path = model_path)\n",
        "\n",
        "  task_to_dataloaders = {}\n",
        "  if 'train' in tasks:\n",
        "    task_to_dataloaders.update({'train' : dataloader_sentences_training})\n",
        "  if 'test' in tasks:\n",
        "    task_to_dataloaders.update({'test' : dataloader_sentences_testing})\n",
        "  if 'scrambled_sentence' in tasks:\n",
        "    dataloader_scrambled_sentences = load_scrambled_sentences(language = interface.language_dir.stem,\n",
        "                                                              interface = interface)\n",
        "    task_to_dataloaders.update({'scrambled_sentence' : dataloader_scrambled_sentences})\n",
        "  if 'noun_list' in tasks:\n",
        "    dataloader_noun_list = load_noun_memory_lists(language = interface.language_dir.stem,\n",
        "                                                  interface = interface)\n",
        "    task_to_dataloaders.update({'noun_list' : dataloader_noun_list})\n",
        "  if 'nonword_list' in tasks:\n",
        "    dataloader_nonword_list = load_nonword_memory_lists(language = memory_list_interface.language_dir.stem,\n",
        "                                                        interface = memory_list_interface,\n",
        "                                                        nonword_legality = nonword_legality)\n",
        "    task_to_dataloaders.update({'nonword_list' : dataloader_nonword_list})\n",
        "  if 'single_nonword_repetition' in tasks:\n",
        "    dataloader_single_nonword_list = load_nonword_memory_lists(language = memory_list_interface.language_dir.stem,\n",
        "                                                               interface = memory_list_interface,\n",
        "                                                               nonword_legality = nonword_legality,\n",
        "                                                               list_length = 1)\n",
        "    task_to_dataloaders.update({'single_nonword_repetition' : dataloader_single_nonword_list})\n",
        "  if 'lexsyn_violation_vb_trn' in tasks:\n",
        "    dataloader_lexsyn_vb_trn = load_lexsyn_violations(language = interface.language_dir.stem,\n",
        "                                                      interface = interface,\n",
        "                                                      violation = 'vb_trn')\n",
        "    task_to_dataloaders.update({'lexsyn_violation_vb_trn' : dataloader_lexsyn_vb_trn})\n",
        "  if 'lexsyn_violation_vb_int' in tasks:\n",
        "    dataloader_lexsyn_vb_int = load_lexsyn_violations(language = interface.language_dir.stem,\n",
        "                                                      interface = interface,\n",
        "                                                      violation = 'vb_int')\n",
        "    task_to_dataloaders.update({'lexsyn_violation_vb_int' : dataloader_lexsyn_vb_int})\n",
        "  if 'lexsyn_violation_vb_inn' in tasks:\n",
        "    dataloader_lexsyn_vb_inn = load_lexsyn_violations(language = interface.language_dir.stem,\n",
        "                                                      interface = interface,\n",
        "                                                      violation = 'vb_inn')\n",
        "    task_to_dataloaders.update({'lexsyn_violation_vb_inn' : dataloader_lexsyn_vb_inn})\n",
        "  if 'lexsyn_violation_snb_inn' in tasks:\n",
        "    datalaoder_lexsyn_snb_inn = load_lexsyn_violations(language = interface.language_dir.stem,\n",
        "                                                       interface = interface,\n",
        "                                                       violation = 'snb_inn')\n",
        "    task_to_dataloaders.update({'lexsyn_violation_snb_inn' : datalaoder_lexsyn_snb_inn})\n",
        "  if 'lexsyn_violation_pvnb_inn' in tasks:\n",
        "    dataloader_lexsyn_pvnb_inn = load_lexsyn_violations(language = interface.language_dir.stem,\n",
        "                                                        interface = interface,\n",
        "                                                        violation = 'pvnb_inn')\n",
        "    task_to_dataloaders.update({'lexsyn_violation_pvnb_inn' : dataloader_lexsyn_pvnb_inn})\n",
        "\n",
        "  return task_to_dataloaders\n",
        "\n",
        "def load_and_test_models(model_id, interface,\n",
        "                         trained_model_epochs = None,\n",
        "                         memory_list_interface = None,\n",
        "                         tasks = ['train', 'test', 'scrambled_sentence', 'noun_list', 'nonword_list', 'single_nonword_repetition',\n",
        "                                 'lexsyn_violation_vb_trn', 'lexsyn_violation_vb_int', 'lexsyn_violation_vb_inn', 'lexsyn_violation_snb_inn', 'lexsyn_violation_pvnb_inn'],\n",
        "                         nonword_legality = 'legal',\n",
        "                         trained_epochs_key = 'trained_epochs',\n",
        "                         report_progress = True):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  model_path = Path.cwd().joinpath(f'model_info/{model_id}')\n",
        "\n",
        "  # Get all models trained across all epochs, or keep the target trained models already in trained_model_epochs\n",
        "  model_training_info = pd.read_csv(model_path.joinpath('model_log.csv'))\n",
        "  if not trained_model_epochs:\n",
        "    trained_model_epochs = unique(list(model_training_info[trained_epochs_key]))\n",
        "\n",
        "  # Read in all dataloaders for all tasks\n",
        "  task_to_dataloaders = load_task_dataloaders(interface = interface, model_path = model_path,\n",
        "                                              memory_list_interface = memory_list_interface,\n",
        "                                              tasks = tasks,\n",
        "                                              nonword_legality = nonword_legality)\n",
        "\n",
        "  # -- Each model is loaded and then tested on different sets of sentences\n",
        "  for trained_epochs in trained_model_epochs:\n",
        "    if report_progress: print(f'{trained_epochs} -- {model_id}')\n",
        "    LM = load_model(interface = interface,\n",
        "                    model_info_path = model_path,\n",
        "                    trained_epochs = trained_epochs)\n",
        "\n",
        "    test_multiple_model_performances(model = LM, task_to_dataloaders = task_to_dataloaders,\n",
        "                                     interface = interface, meta_info = {'trained_epochs' : trained_epochs})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J-IM4D7BWue"
      },
      "source": [
        "## Examine specific model behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOyOj3c1MbMe",
        "outputId": "d20527f2-70b3-421f-991b-1610f296b65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199 -- lm_129_2023-04-18_853720c5-c833-4e01-9e37-20e8960a80da\n"
          ]
        }
      ],
      "source": [
        "model_id = \"lm_129_2023-04-18_853720c5-c833-4e01-9e37-20e8960a80da\"\n",
        "\n",
        "nonword_legality = 'legal'\n",
        "nonword_memory_list_interface = LMInterface.LMInterface(language_dir = language_dir,\n",
        "                                                        phonology_filestem = f'phonology_artificial_{nonword_legality}_nonwords')\n",
        "\n",
        "#tasks = ['train', 'test', 'scrambled_sentence', 'nonword_list', 'single_nonword_repetition', 'lexsyn_violation_vb_trn', 'lexsyn_violation_vb_int', 'lexsyn_violation_vb_inn', 'lexsyn_violation_snb_inn', 'lexsyn_violation_pvnb_inn']\n",
        "#tasks = ['lexsyn_violation_vb_trn', 'lexsyn_violation_vb_int', 'lexsyn_violation_vb_inn', 'lexsyn_violation_snb_inn', 'lexsyn_violation_pvnb_inn']\n",
        "tasks = ['noun_list']\n",
        "\n",
        "load_and_test_models(model_id = model_id,\n",
        "                     interface = interface,\n",
        "                     trained_model_epochs = ['199'],\n",
        "                     memory_list_interface = nonword_memory_list_interface,\n",
        "                     tasks = tasks,\n",
        "                     nonword_legality = nonword_legality)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bklb2BxCfw-R",
        "tOFWKY5HoeHO",
        "J27zDfW1c2wc"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMTbuLXTgeFZb7Y4TyBgVRt",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}